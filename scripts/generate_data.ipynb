{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate synthetic data\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed46d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pathlib\n",
    "import random \n",
    "import pandas as pd\n",
    "\n",
    "from utils.async_req import get_llm_responses_openai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f590255b",
   "metadata": {},
   "source": [
    "## System prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617fc8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create user prompt\n",
    "input_prompt =\\\n",
    "\"\"\"\n",
    "You are a data-generation assistant.\n",
    "\n",
    "TASK:\n",
    "Produce a **single JSON array**. It must contain exactly 50 objects, one for each semantic category you invent.\n",
    "\n",
    "For each object.\n",
    "1. Pick a simple, universally-understood category label (e.g. \"fruit\", \"animals\" - don't recycle these specific ones).\n",
    "2. Choose an integer `list_length` uniformly at random in [4, 8].\n",
    "3. Choose an integer `category_length` uniformly at random in [1, list_length - 1].\n",
    "4. Let `noncategory_length` = list_length - category_length.\n",
    "5. Construct:\n",
    "    - `category_list`: `category_length` distinct single-word items that belong to the category.\n",
    "    - `noncategory_list`: `noncategory_length` distinct single-words items that do **not** belong to the category. (It is okay if some of these happen to be valid members of *other* categories.)\n",
    "\n",
    "CONSTRAINTS:\n",
    "- All words must be lowercase, alphabetic, and contain no spaces or punctuation or special characters.\n",
    "- No duplicate words within a single object.\n",
    "- Exactly 50 objects total.\n",
    "- Vary the categories.\n",
    "- The 50 objects should collectively cover the full range of `list_length` and `category_length` values.\n",
    "- Remember category_length + noncategory_length must be between 4 and 8; both category_length and noncategory_length should be nonzero.\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Return a JSON in the following format:\n",
    "```\n",
    "{'categories': [\n",
    "    {\n",
    "        \"category\": \"animals\",\n",
    "        \"category_length\": 3,\n",
    "        \"noncategory_length\": 2,\n",
    "        \"category_list\": [\"cat\", \"dog\", \"mouse\"],\n",
    "        \"noncategory_list\": [\"cherry\", \"bus\"]\n",
    "    },\n",
    "    ...\n",
    "]}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25448ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send requests\n",
    "llm_responses = await get_llm_responses_openai(\n",
    "    [[{'role': 'user', 'content': input_prompt}]] * 100, # Send this 50 times = 2.5k examples\n",
    "    params = {'model': 'o4-mini-2025-04-16', 'response_format': {'type': 'json_object'}},\n",
    "    batch_size = 10, # Send 5 async at once, 50/10 = 5 seperate batches\n",
    "    api_key = os.environ.get('OPENAI_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec43959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse + output data\n",
    "def make_corrupted(cat_words, non_words, full_list):\n",
    "    \"\"\"\n",
    "    Replace one category word with a noncat distractor.\n",
    "    \"\"\"\n",
    "    cat_pos = random.choice([i for i, w in enumerate(full_list) if w in cat_words])\n",
    "\n",
    "    # Find a replacement that is NOT in the category and NOT already in the list\n",
    "    pool = set(non_words) | {\"stone\",\"chair\",\"lamp\",\"cloud\",\"plate\"}\n",
    "    pool -= set(full_list)           # avoid duplicates\n",
    "    repl = random.choice(sorted(pool))\n",
    "\n",
    "    corrupted = full_list.copy()\n",
    "    corrupted[cat_pos] = repl\n",
    "    return corrupted\n",
    "\n",
    "def parse_response(response):\n",
    "    \"\"\"\n",
    "    Parse response and shuffle them!\n",
    "    \"\"\"\n",
    "    try:\n",
    "        raw_content = response['choices'][0]['message']['content'].strip()\n",
    "        cats = json.loads(raw_content)['categories']\n",
    "        \n",
    "        processed = []\n",
    "        sample_ix = 0\n",
    "        for obj in cats:\n",
    "\n",
    "            # ----- Basic schema -----\n",
    "            cat_len  = obj.get('category_length')\n",
    "            non_len  = obj.get('noncategory_length')\n",
    "            cat_lst  = obj.get('category_list')\n",
    "            non_lst  = obj.get('noncategory_list')\n",
    "\n",
    "            # ----- Basic checks -----\n",
    "            if (not all(isinstance(x, int) for x in (cat_len, non_len))\n",
    "                or cat_len != len(cat_lst)\n",
    "                or non_len != len(non_lst)\n",
    "                or not 4 <= cat_len + non_len <= 8\n",
    "                or len(set(cat_lst + non_lst)) != cat_len + non_len): # Throw malformed, outside length bounds, duplicate words\n",
    "                continue\n",
    "\n",
    "            if cat_len == 0 or non_len == 0: # Throw out stuff with 1 noncat elements - needed to make a corrupted pair\n",
    "                continue\n",
    "\n",
    "            # ----- Holdout -----\n",
    "            held_out_non = random.choice(non_lst)\n",
    "            clean_non_lst = [w for w in non_lst if w != held_out_non]\n",
    "\n",
    "            # ----- Shuffle -----\n",
    "            clean_list = cat_lst + clean_non_lst\n",
    "            random.shuffle(clean_list)\n",
    "            clean_cat_idx = [i for i, w in enumerate(clean_list) if w in set(cat_lst)]\n",
    "\n",
    "            # ----- Corrupted version -----\n",
    "            cat_pos = random.choice(clean_cat_idx)\n",
    "            corrupt_list = clean_list.copy()\n",
    "            corrupt_list[cat_pos] = held_out_non\n",
    "            corrupt_cat_idx = [i for i, w in enumerate(corrupt_list) if w in set(cat_lst)]\n",
    "\n",
    "            processed.append(\n",
    "                {\n",
    "                    'sample_ix': sample_ix,\n",
    "                    'category': obj['category'],\n",
    "\n",
    "                    # 'category_list': cat_lst,\n",
    "                    # 'category_length': cat_len,\n",
    "                    # 'noncategory_list': non_lst,\n",
    "                    # 'noncategory_length': non_len,\n",
    "                    'list_length': len(clean_list),\n",
    "\n",
    "                    # Clean versions\n",
    "                    'clean_list': clean_list,\n",
    "                    'clean_category_indices': clean_cat_idx,\n",
    "                    'clean_category_count': len(clean_cat_idx),\n",
    "\n",
    "                    # Corrupt versions\n",
    "                    'corrupt_list': corrupt_list,\n",
    "                    'corrupt_category_indices': corrupt_cat_idx,\n",
    "                    'corrupt_category_count': len(corrupt_cat_idx)\n",
    "                }\n",
    "            )\n",
    "\n",
    "            sample_ix += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Parse response error: {e}\")\n",
    "        processed = []\n",
    "\n",
    "    return processed\n",
    "\n",
    "final_output = []\n",
    "for response in llm_responses:\n",
    "    final_output.extend(parse_response(response))\n",
    "print(len(final_output))\n",
    "\n",
    "# Save\n",
    "path = pathlib.Path('./synthetic-data.json')\n",
    "path.write_text(json.dumps(final_output, indent = 2))\n",
    "print(f\"Wrote {path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f6e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick dataset quality checks \n",
    "final_df = pd.DataFrame(final_output)\n",
    "\n",
    "print(f'Samples: {len(final_df)}')\n",
    "print(f'Unique categories: {len(final_df[['category']].drop_duplicates())}')\n",
    "display(final_df.groupby('list_length', as_index = False).agg(sum = ('category', 'count')))\n",
    "display(final_df.groupby(['list_length', 'clean_category_count'], as_index = False).agg(sum = ('category', 'count')).sort_values(by = ['list_length', 'clean_category_count']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c7c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dogs: [whippet, pug]\n",
    "# mammals: [whale, camel]\n",
    "\n",
    "\n",
    "# list: [whale, camel, whippet, pug]\n",
    "# mammal_indices: [0, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
