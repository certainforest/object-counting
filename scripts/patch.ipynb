{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07787527",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Causal patching\n",
    "\"\"\"\n",
    "None # fun trick hehe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40be6dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: NVIDIA L40S\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 44.42 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import importlib\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from tqdm import tqdm\n",
    "import utils\n",
    "\n",
    "from utils.mem import check_memory\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "seed = 123\n",
    "# torch.cuda.empty_cache()\n",
    "# clear_all_cuda_memory()\n",
    "check_memory()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b8d74",
   "metadata": {},
   "source": [
    "## Load model & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c78d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5439b894484f59b06e095f88bfc7fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load qwen-14B – would ideally use 32B if not constrained by pod seup\n",
    "model_path = \"/workspace/models/Qwen3-14B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype = torch.bfloat16, trust_remote_code = True).to(device) # if you check config.json it defaults to bfloat16: https://huggingface.co/Qwen/Qwen3-32B/blob/main/config.json; probably just large\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token = False, add_bos_token = False, trust_remote_code = True) # right padding by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ab132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_ix</th>\n",
       "      <th>variant</th>\n",
       "      <th>category</th>\n",
       "      <th>list_length</th>\n",
       "      <th>full_list</th>\n",
       "      <th>category_indices</th>\n",
       "      <th>category_count</th>\n",
       "      <th>category_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>clean</td>\n",
       "      <td>colors</td>\n",
       "      <td>3</td>\n",
       "      <td>[red, banana, computer]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>1</td>\n",
       "      <td>[red]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>clean</td>\n",
       "      <td>vehicles</td>\n",
       "      <td>3</td>\n",
       "      <td>[car, bus, banana]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>2</td>\n",
       "      <td>[car, bus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>clean</td>\n",
       "      <td>shapes</td>\n",
       "      <td>3</td>\n",
       "      <td>[square, circle, triangle]</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>3</td>\n",
       "      <td>[square, circle, triangle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>clean</td>\n",
       "      <td>continents</td>\n",
       "      <td>3</td>\n",
       "      <td>[computer, asia, apple]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>1</td>\n",
       "      <td>[asia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>clean</td>\n",
       "      <td>metals</td>\n",
       "      <td>3</td>\n",
       "      <td>[banana, gold, silver]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>[gold, silver]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>44</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>street_signs</td>\n",
       "      <td>7</td>\n",
       "      <td>[rose, car, yield, speed, apple, dog, pedestrian]</td>\n",
       "      <td>[2, 3, 6]</td>\n",
       "      <td>3</td>\n",
       "      <td>[yield, speed, pedestrian]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>45</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>continents</td>\n",
       "      <td>7</td>\n",
       "      <td>[asia, dog, africa, europe, apple, car, americas]</td>\n",
       "      <td>[0, 2, 3, 6]</td>\n",
       "      <td>4</td>\n",
       "      <td>[asia, africa, europe, americas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>46</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>time_units</td>\n",
       "      <td>7</td>\n",
       "      <td>[apple, year, car, day, month, minute, hour]</td>\n",
       "      <td>[1, 3, 4, 5, 6]</td>\n",
       "      <td>5</td>\n",
       "      <td>[year, day, month, minute, hour]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>47</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>elements</td>\n",
       "      <td>7</td>\n",
       "      <td>[hydrogen, apple, gold, silver, helium, oxygen...</td>\n",
       "      <td>[0, 2, 3, 4, 5, 6]</td>\n",
       "      <td>6</td>\n",
       "      <td>[hydrogen, gold, silver, helium, oxygen, nitro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>48</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>shapes</td>\n",
       "      <td>7</td>\n",
       "      <td>[apple, car, dog, rose, chair, table, house]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9978 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_ix  variant      category  list_length  \\\n",
       "0             0    clean        colors            3   \n",
       "1             1    clean      vehicles            3   \n",
       "2             2    clean        shapes            3   \n",
       "3             3    clean    continents            3   \n",
       "4             4    clean        metals            3   \n",
       "...         ...      ...           ...          ...   \n",
       "9973         44  corrupt  street_signs            7   \n",
       "9974         45  corrupt    continents            7   \n",
       "9975         46  corrupt    time_units            7   \n",
       "9976         47  corrupt      elements            7   \n",
       "9977         48  corrupt        shapes            7   \n",
       "\n",
       "                                              full_list    category_indices  \\\n",
       "0                               [red, banana, computer]                 [0]   \n",
       "1                                    [car, bus, banana]              [0, 1]   \n",
       "2                            [square, circle, triangle]           [0, 1, 2]   \n",
       "3                               [computer, asia, apple]                 [1]   \n",
       "4                                [banana, gold, silver]              [1, 2]   \n",
       "...                                                 ...                 ...   \n",
       "9973  [rose, car, yield, speed, apple, dog, pedestrian]           [2, 3, 6]   \n",
       "9974  [asia, dog, africa, europe, apple, car, americas]        [0, 2, 3, 6]   \n",
       "9975       [apple, year, car, day, month, minute, hour]     [1, 3, 4, 5, 6]   \n",
       "9976  [hydrogen, apple, gold, silver, helium, oxygen...  [0, 2, 3, 4, 5, 6]   \n",
       "9977       [apple, car, dog, rose, chair, table, house]                  []   \n",
       "\n",
       "      category_count                                     category_words  \n",
       "0                  1                                              [red]  \n",
       "1                  2                                         [car, bus]  \n",
       "2                  3                         [square, circle, triangle]  \n",
       "3                  1                                             [asia]  \n",
       "4                  2                                     [gold, silver]  \n",
       "...              ...                                                ...  \n",
       "9973               3                         [yield, speed, pedestrian]  \n",
       "9974               4                   [asia, africa, europe, americas]  \n",
       "9975               5                   [year, day, month, minute, hour]  \n",
       "9976               6  [hydrogen, gold, silver, helium, oxygen, nitro...  \n",
       "9977               0                                                 []  \n",
       "\n",
       "[9978 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load synthetic data – this time splitting the clean + corrupt (don't love this verbiage...) variants (should 2x length)\n",
    "with open('/workspace/data/synthetic-data-1.json', 'r') as file:\n",
    "    raw_df = json.load(file)\n",
    "\n",
    "raw_df = pd.DataFrame(raw_df)\n",
    "\n",
    "input_df =\\\n",
    "    pd.concat([\n",
    "        raw_df\\\n",
    "            .rename(columns = {'clean_list': 'full_list', 'clean_category_indices': 'category_indices', 'clean_category_count': 'category_count'})\\\n",
    "            .assign(variant = 'clean')\\\n",
    "            [['sample_ix', 'variant', 'category', 'list_length', 'full_list', 'category_indices', 'category_count']],\n",
    "        raw_df\\\n",
    "            .rename(columns = {'corrupt_list': 'full_list', 'corrupt_category_indices': 'category_indices', 'corrupt_category_count': 'category_count'})\\\n",
    "            .assign(variant = 'corrupt')\\\n",
    "            [['sample_ix', 'variant', 'category', 'list_length', 'full_list', 'category_indices', 'category_count']]\n",
    "    ], ignore_index = True)\\\n",
    "    .assign(\n",
    "        category_words = lambda df: df.apply(lambda row: [row['full_list'][i] for i in row['category_indices']], axis = 1),\n",
    "    )\n",
    "\n",
    "# input_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccd4c266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_ix</th>\n",
       "      <th>variant</th>\n",
       "      <th>category</th>\n",
       "      <th>list_length</th>\n",
       "      <th>full_list</th>\n",
       "      <th>category_indices</th>\n",
       "      <th>category_count</th>\n",
       "      <th>category_words</th>\n",
       "      <th>prompt</th>\n",
       "      <th>token_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sample_ix, variant, category, list_length, full_list, category_indices, category_count, category_words, prompt, token_length]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m input_df = input_df.pipe(\u001b[38;5;28;01mlambda\u001b[39;00m df: df[df[\u001b[33m'\u001b[39m\u001b[33msample_ix\u001b[39m\u001b[33m'\u001b[39m].isin(sample_ix_with_same_length)]).reset_index(drop = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     46\u001b[39m display(input_df)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43minput_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# tokenize + prep data \n",
    "# todo: move function into module\n",
    "def prep_prompt(category, choices, tokenizer):\n",
    "    choices_str = ' '.join(choices)\n",
    "    user_prompt = f\"Count the number of words in the following  list that match the given type, and put the numerical answer in parentheses.\\n\\nType: {category}\\nList: [ {choices_str} ]\"\n",
    "    \n",
    "    instruct_formatted_prompt = tokenizer.apply_chat_template(\n",
    "        [{'role': 'user', 'content': user_prompt}, {'role': 'assistant', 'content': 'Answer: ('}],\n",
    "        tokenize = False,\n",
    "        add_generation_prompt = False,\n",
    "        continue_final_message = True # Otherwise appends eos token\n",
    "    )\n",
    "\n",
    "    return instruct_formatted_prompt\n",
    "\n",
    "# Add the prompt\n",
    "input_df =\\\n",
    "    input_df\\\n",
    "    .assign(prompt = lambda df: df.apply(lambda row: prep_prompt(row['category'], row['full_list'], tokenizer), axis = 1))\n",
    "\n",
    "# Add the token lengths\n",
    "token_lengths = tokenizer(\n",
    "    input_df['prompt'].tolist(),\n",
    "    add_special_tokens = False,\n",
    "    max_length = 128,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    return_tensors = 'pt'\n",
    "    )['attention_mask'].sum(dim = 1).tolist()\n",
    "\n",
    "input_df =\\\n",
    "    input_df\\\n",
    "    .assign(token_length = token_lengths)\n",
    "\n",
    "# Filter only for sample_ix such that the two variants have the same token length, this will simplify patching\n",
    "sample_ix_with_same_length =\\\n",
    "    input_df\\\n",
    "    .groupby('sample_ix', as_index = False)\\\n",
    "    .agg(unique_lengths = ('token_length', 'nunique'))\\\n",
    "    .pipe(lambda df: df[df['unique_lengths'] == 1])\\\n",
    "    ['sample_ix']\\\n",
    "    .tolist()\n",
    "\n",
    "input_df = input_df.pipe(lambda df: df[df['sample_ix'].isin(sample_ix_with_same_length)]).reset_index(drop = True)\n",
    "\n",
    "display(input_df)\n",
    "print(input_df['prompt'].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff9c94fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(72)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "word_start_tok_mask has 6 starts but list has 7 words (sample 1545).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokenized_prompts[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m].sum(dim = \u001b[32m1\u001b[39m).max()) \u001b[38;5;66;03m# Must be under max length to confirm nothing was truncated (since attention mask applies a 1 to indicate \"some guy was here\")\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Create and chunk into lists of size 1k each - these will be the export breaks\u001b[39;00m\n\u001b[32m     11\u001b[39m test_dl = DataLoader(\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[43mReconstructableTextDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_ix\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msample_ix\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvariant\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfull_lists\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfull_list\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcategory_indices_list\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcategory_indices\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     21\u001b[39m     batch_size = \u001b[32m16\u001b[39m,\n\u001b[32m     22\u001b[39m     shuffle = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     23\u001b[39m     collate_fn = stack_collate\n\u001b[32m     24\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/object-counting/scripts/../utils/dataset.py:139\u001b[39m, in \u001b[36mReconstructableTextDataset.__init__\u001b[39m\u001b[34m(self, raw_texts, tokenizer, max_length, full_lists, category_indices_list, **identifiers)\u001b[39m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis_category_tok not subset of list_tok_mask (sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wstart.sum().item() != \u001b[38;5;28mlen\u001b[39m(words):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mword_start_tok_mask has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwstart.sum().item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m starts but list has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(words)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m words (sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# --- 6) pack tensors ---\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[38;5;28mself\u001b[39m.running_counts.append(torch.tensor(run_cnt, dtype=torch.long))\n",
      "\u001b[31mValueError\u001b[39m: word_start_tok_mask has 6 starts but list has 7 words (sample 1545)."
     ]
    }
   ],
   "source": [
    "# create dataloader\n",
    "import utils.dataset\n",
    "importlib.reload(utils.dataset)\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_prompts = tokenizer(input_df['prompt'].tolist(), add_special_tokens = False, max_length = 80, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "print(tokenized_prompts['attention_mask'].sum(dim = 1).max()) # Must be under max length to confirm nothing was truncated (since attention mask applies a 1 to indicate \"some guy was here\")\n",
    "\n",
    "# Create and chunk into lists of size 1k each - these will be the export breaks\n",
    "test_dl = DataLoader(\n",
    "    ReconstructableTextDataset(\n",
    "        input_df['prompt'].tolist(),\n",
    "        tokenizer,\n",
    "        max_length = 128,\n",
    "        sample_ix = input_df['sample_ix'].tolist(),\n",
    "        variant = input_df['variant'].tolist(),\n",
    "        full_lists = input_df['full_list'].tolist(),\n",
    "        category_indices_list = input_df['category_indices'].tolist()\n",
    "        ),\n",
    "    batch_size = 16,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca2559",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2ffde43",
   "metadata": {},
   "source": [
    "## Quick evals\n",
    "Identify a set of questions where both the corrupt + correct are accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5eabea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 55/1248 [00:06<02:21,  8.44it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[123]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m variants = batch[\u001b[33m'\u001b[39m\u001b[33mvariant\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     12\u001b[39m     seq_ends = attention_mask.sum(dim = \u001b[32m1\u001b[39m) - \u001b[32m1\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sample_indices)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:730\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    725\u001b[39m output_hidden_states = (\n\u001b[32m    726\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    727\u001b[39m )\n\u001b[32m    729\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    743\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    744\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:463\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    461\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py:48\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:300\u001b[39m, in \u001b[36mQwen3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    298\u001b[39m residual = hidden_states\n\u001b[32m    299\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    303\u001b[39m outputs = (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:90\u001b[39m, in \u001b[36mQwen3MLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     down_proj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# run fwd. pass\n",
    "output_records = []\n",
    "\n",
    "for batch_ix, batch in tqdm(enumerate(test_dl), total = len(test_dl)):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    sample_indices = batch['sample_ix']\n",
    "    variants = batch['variant']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)['logits']\n",
    "        seq_ends = attention_mask.sum(dim = 1) - 1\n",
    "\n",
    "    for i in range(len(sample_indices)):\n",
    "        last_pos = seq_ends[i].item()\n",
    "        last_logits = logits[i, last_pos, :]\n",
    "        probs = torch.softmax(last_logits, dim = -1)\n",
    "        top_token = torch.argmax(probs).item()\n",
    "        top_prob = probs[top_token].item()\n",
    "\n",
    "        output_records.append(\n",
    "            {\n",
    "                'sample_ix': int(sample_indices[i]),\n",
    "                'variant': variants[i],\n",
    "                'output_token': tokenizer.decode(top_token),\n",
    "                'output_prob': top_prob,\n",
    "            }\n",
    "        )\n",
    "\n",
    "output_df = pd.DataFrame(output_records) # todo: there's a bug w/ the data loader, causing sample_ix to repeat intra-batch..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dd6d5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: fix sample index, not advised \n",
    "input_df['sample_ix'] = input_df.groupby('variant').cumcount()\n",
    "output_df['sample_ix'] = output_df.groupby('variant').cumcount()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c2a7e3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_ix</th>\n",
       "      <th>variant</th>\n",
       "      <th>category_count</th>\n",
       "      <th>category</th>\n",
       "      <th>output_token</th>\n",
       "      <th>output_prob</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>is_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>clean</td>\n",
       "      <td>1</td>\n",
       "      <td>colors</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>clean</td>\n",
       "      <td>2</td>\n",
       "      <td>vehicles</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>clean</td>\n",
       "      <td>3</td>\n",
       "      <td>shapes</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>clean</td>\n",
       "      <td>1</td>\n",
       "      <td>continents</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>clean</td>\n",
       "      <td>2</td>\n",
       "      <td>metals</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>4984</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>3</td>\n",
       "      <td>street_signs</td>\n",
       "      <td>2</td>\n",
       "      <td>0.941406</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>4985</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>4</td>\n",
       "      <td>continents</td>\n",
       "      <td>4</td>\n",
       "      <td>0.976562</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>4986</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>5</td>\n",
       "      <td>time_units</td>\n",
       "      <td>3</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>4987</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>6</td>\n",
       "      <td>elements</td>\n",
       "      <td>4</td>\n",
       "      <td>0.851562</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>4988</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>0</td>\n",
       "      <td>shapes</td>\n",
       "      <td>2</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9978 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_ix  variant  category_count      category output_token  \\\n",
       "0             0    clean               1        colors            1   \n",
       "1             1    clean               2      vehicles            2   \n",
       "2             2    clean               3        shapes            3   \n",
       "3             3    clean               1    continents            1   \n",
       "4             4    clean               2        metals            2   \n",
       "...         ...      ...             ...           ...          ...   \n",
       "9973       4984  corrupt               3  street_signs            2   \n",
       "9974       4985  corrupt               4    continents            4   \n",
       "9975       4986  corrupt               5    time_units            3   \n",
       "9976       4987  corrupt               6      elements            4   \n",
       "9977       4988  corrupt               0        shapes            2   \n",
       "\n",
       "      output_prob  is_correct  is_int  \n",
       "0        1.000000           1       1  \n",
       "1        1.000000           1       1  \n",
       "2        1.000000           1       1  \n",
       "3        1.000000           1       1  \n",
       "4        1.000000           1       1  \n",
       "...           ...         ...     ...  \n",
       "9973     0.941406           0       1  \n",
       "9974     0.976562           1       1  \n",
       "9975     0.996094           0       1  \n",
       "9976     0.851562           0       1  \n",
       "9977     0.992188           0       1  \n",
       "\n",
       "[9978 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy rate: 0.7928\n"
     ]
    }
   ],
   "source": [
    "# get accuracy rate for each (sample_ix, variant)\n",
    "accurate_rates =\\\n",
    "    input_df[['sample_ix', 'variant', 'category_count', 'category']]\\\n",
    "    .merge(output_df, on = ['sample_ix', 'variant'], how = 'inner')\\\n",
    "    .assign(is_correct = lambda df:  np.where(df['output_token'].str.strip() == df['category_count'].astype(str).str.strip(), 1, 0))\\\n",
    "    .assign(is_int = lambda df: np.where(df['output_token'].isin(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']), 1, 0))\n",
    "\n",
    "display(accurate_rates)\n",
    "print(f\"Accuracy rate: {accurate_rates['is_correct'].mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9ba49fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3418\n"
     ]
    }
   ],
   "source": [
    "# get only sample_ix's where both questions were accurate\n",
    "sample_indices_for_patching =\\\n",
    "    accurate_rates\\\n",
    "    .groupby('sample_ix', as_index = False)\\\n",
    "    .agg(n_correct = ('is_correct', 'sum'))\\\n",
    "    .pipe(lambda df: df[df['n_correct'] == 2])\\\n",
    "    ['sample_ix'].tolist()\n",
    "\n",
    "print(len(sample_indices_for_patching))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ff1d13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_df = input_df.pipe(lambda df: df[df['sample_ix'].isin(sample_indices_for_patching)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3e51c",
   "metadata": {},
   "source": [
    "## Patching time\n",
    "first collect hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4fbe5dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch imnputs\n",
    "patch_df = input_df.pipe(lambda df: df[df['sample_ix'].isin(sample_indices_for_patching)])\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    ReconstructableTextDataset(\n",
    "        patch_df['prompt'].tolist(),\n",
    "        tokenizer,\n",
    "        max_length = 80,\n",
    "        sample_ix = patch_df['sample_ix'].tolist(),\n",
    "        variant = patch_df['variant'].tolist(),\n",
    "        full_list = patch_df['full_list'].tolist(),\n",
    "        category_indices = patch_df['category_indices'].tolist()\n",
    "        ),\n",
    "    batch_size = 8,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "978d4962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/855 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'cum_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[116]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m     mask_i  = list_masks[i].bool()\n\u001b[32m     25\u001b[39m     hs_sub  = hs[i, :, mask_i, :] \u001b[38;5;66;03m# K × Nₗ × D\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     cum_sub = \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcum_count\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[i][mask_i]\n\u001b[32m     28\u001b[39m     output_records.append(\n\u001b[32m     29\u001b[39m         {\n\u001b[32m     30\u001b[39m             \u001b[33m'\u001b[39m\u001b[33msample_ix\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(sample_indices[i]),\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m         }\n\u001b[32m     41\u001b[39m     )\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_ix >= \u001b[32m500\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'cum_count'"
     ]
    }
   ],
   "source": [
    "output_records = []\n",
    "\n",
    "for batch_ix, batch in tqdm(enumerate(test_dl), total = len(test_dl)):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    sample_indices = batch['sample_ix']\n",
    "    variants = batch['variant']\n",
    "    list_masks = batch['attention_mask'] # BoolTensor B × N\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids, attention_mask, output_hidden_states = True)\n",
    "        hs = torch.stack(out.hidden_states, dim = 1).detach()[:, 1:, :, :].float() # B x K x N x D - skip first hs which is embed layer\n",
    "        logits = out.logits\n",
    "        seq_ends = attention_mask.sum(dim = 1) - 1\n",
    "\n",
    "    for i in range(len(sample_indices)):\n",
    "        last_pos = seq_ends[i].item()\n",
    "        last_logits = logits[i, last_pos, :]\n",
    "        probs = torch.softmax(last_logits, dim = -1)\n",
    "        top_token = torch.argmax(probs).item()\n",
    "        top_prob = probs[top_token].item()\n",
    "\n",
    "        # ── keep only list-token activations ───────────────────────────\n",
    "        mask_i  = list_masks[i].bool()\n",
    "        hs_sub  = hs[i, :, mask_i, :] # K × Nₗ × D\n",
    "        cum_sub = batch['cum_count'][i][mask_i]\n",
    "\n",
    "        output_records.append(\n",
    "            {\n",
    "                'sample_ix': int(sample_indices[i]),\n",
    "                'variant': variants[i],\n",
    "                'output_token': tokenizer.decode(top_token),\n",
    "                'output_prob': top_prob,\n",
    "                'hidden_states': hs_sub.cpu(), # K x N x D\n",
    "                'match_flag': batch['match_flag'].tolist(),\n",
    "                'original_tokens': batch['original_tokens'],\n",
    "                'list_mask': mask_i, # N\n",
    "                'cum_count': cum_sub, # N\n",
    "                'word_token_spans': batch['word_token_spans'][i] # Store for patching\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if batch_ix >= 500:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d99659a",
   "metadata": {},
   "source": [
    "## Probe sweep - pick the mediator layer $l^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683fd5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "import random \n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0 — collect clean-only records into per-layer bags\n",
    "# ------------------------------------------------------------\n",
    "K = output_records[0][\"hidden_states\"].shape[0]   # # layers\n",
    "rng = random.Random(42)\n",
    "# MAX_ROWS = 10_000 # Per split, per\n",
    "alpha = 10 # Choose such that tr/test loss is similar\n",
    "\n",
    "# Split by sample_ix so paired clean/corrupt never leak\n",
    "clean_records = [r for r in output_records if r[\"variant\"] == \"clean\"]\n",
    "sample_ids = sorted({r[\"sample_ix\"] for r in clean_records})\n",
    "rng.shuffle(sample_ids)\n",
    "split = int(0.8 * len(sample_ids))\n",
    "train_ids, test_ids = set(sample_ids[:split]), set(sample_ids[split:])\n",
    "\n",
    "K = clean_records[0][\"hidden_states\"].shape[0]\n",
    "bags = {l: {\"train_X\": [], \"train_y\": [], \"test_X\": [], \"test_y\": []} for l in range(K)}\n",
    "\n",
    "for rec in clean_records:\n",
    "    bag = (\n",
    "        \"train_\" if rec[\"sample_ix\"] in train_ids else \"test_\"\n",
    "    )\n",
    "    hs = rec[\"hidden_states\"].numpy()        # K × Nₗ × D  (already CPU)\n",
    "    y  = rec[\"cum_count\"].numpy()            # Nₗ\n",
    "    for l in range(K):\n",
    "        bags[l][bag + \"X\"].append(hs[l])     # each is (Nₗ, D)\n",
    "        bags[l][bag + \"y\"].append(y)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1 — stack into 2-D matrices per layer\n",
    "# ------------------------------------------------------------\n",
    "Xs_tr, ys_tr, Xs_te, ys_te = [], [], [], []\n",
    "for l in range(K):\n",
    "    X_tr = np.concatenate(bags[l][\"train_X\"])   # (∑Nₗ, D)\n",
    "    y_tr = np.concatenate(bags[l][\"train_y\"])\n",
    "    X_te = np.concatenate(bags[l][\"test_X\"])\n",
    "    y_te = np.concatenate(bags[l][\"test_y\"])\n",
    "\n",
    "    Xs_tr.append(X_tr)\n",
    "    ys_tr.append(y_tr)\n",
    "    Xs_te.append(X_te)\n",
    "    ys_te.append(y_te)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2 — train one Ridge probe per layer, evaluate on held-out set\n",
    "# ------------------------------------------------------------\n",
    "r2_test, probes = [], []\n",
    "for l in range(K):\n",
    "    reg = Ridge(alpha = alpha, solver = 'auto', fit_intercept=True)\n",
    "    reg.fit(Xs_tr[l], ys_tr[l])\n",
    "\n",
    "    y_hat_tr = reg.predict(Xs_tr[l])\n",
    "    y_hat_te = reg.predict(Xs_te[l])\n",
    "    R2_train = r2_score(reg.predict(Xs_tr[l]),  ys_tr[l])\n",
    "\n",
    "    r2_tr = r2_score(ys_tr[l], y_hat_tr)\n",
    "    r2_te = r2_score(ys_te[l], y_hat_te)\n",
    "\n",
    "    r2_test.append(r2_te)\n",
    "    probes.append(reg)\n",
    "\n",
    "    print(f\"Layer {l:2d} | train R² {r2_tr:6.3f} | test R² {r2_te:6.3f}\")\n",
    "\n",
    "print(\"\\nLayer  |  Test R²\")\n",
    "for l, r2 in enumerate(r2_test):\n",
    "    print(f\"{l:2d}     {r2:6.3f}\")\n",
    "\n",
    "layer_star = int(np.argmax(r2_test))\n",
    "print(f\"\\nMediator candidate layer = {layer_star}  (R² = {r2_test[layer_star]:.3f})\")\n",
    "\n",
    "w_star = torch.tensor(probes[layer_star].coef_, dtype=torch.float32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
