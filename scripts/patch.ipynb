{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07787527",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Causal patching\n",
    "\"\"\"\n",
    "None # fun trick hehe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40be6dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CUDA memory cleared on all devices.\n",
      "Device 0: NVIDIA L40S\n",
      "  Allocated: 29.86 GB\n",
      "  Reserved: 30.60 GB\n",
      "  Total: 44.42 GB\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import nnsight\n",
    "import importlib\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from tqdm import tqdm\n",
    "import utils\n",
    "\n",
    "from utils.mem import check_memory, clear_all_cuda_memory\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "seed = 123\n",
    "# torch.cuda.empty_cache()\n",
    "clear_all_cuda_memory()\n",
    "check_memory()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b8d74",
   "metadata": {},
   "source": [
    "## Load model & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c78d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2540e60b3fff4956a57a46cad3b6c1a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load qwen-14B – would ideally use 32B if not constrained by pod seup\n",
    "model_path = \"/workspace/models/Qwen3-14B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype = torch.bfloat16, trust_remote_code = True).to(device) # if you check config.json it defaults to bfloat16: https://huggingface.co/Qwen/Qwen3-32B/blob/main/config.json; probably just large\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token = False, add_bos_token = False, trust_remote_code = True) # right padding by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf827d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden states layers (post-layer): 40\n",
      "Hidden state size (post-layer): torch.Size([2, 64, 5120])\n",
      "Hidden state size (MLP output): torch.Size([2, 64, 5120])\n",
      "Hidden state size (Attention output): torch.Size([2, 64, 5120])\n"
     ]
    }
   ],
   "source": [
    "# Load reverse-engineered forward pass function that returns the MLP output, attention output, and final layer hidden state at each layer\n",
    "from utils.qwen3 import run_qwen3\n",
    "\n",
    "def test_custom_forward_pass(model):\n",
    "    inputs = tokenizer(['Hi! I am a dog and I like to bark', 'Vegetables are good for'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 64).to(model.device)\n",
    "    original_results = model(**inputs)\n",
    "    custom_results = run_qwen3(model, inputs['input_ids'], inputs['attention_mask'], return_hidden_states = True)\n",
    "    assert torch.equal(original_results.logits, custom_results['logits']), 'Error in custom forward' # Checks that this function replicates the original model exactly\n",
    "    print(f\"Hidden states layers (post-layer): {len(custom_results['layer_out_hidden_states'])}\")\n",
    "    print(f\"Hidden state size (post-layer): {(custom_results['layer_out_hidden_states'][0].shape)}\")\n",
    "    print(f\"Hidden state size (MLP output): {(custom_results['mlp_out_hidden_states'][0].shape)}\")\n",
    "    print(f\"Hidden state size (Attention output): {(custom_results['attn_out_hidden_states'][0].shape)}\")\n",
    "\n",
    "test_custom_forward_pass(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e43ab132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load synthetic data – this time splitting the clean + corrupt (don't love this verbiage...) variants (should 2x length)\n",
    "with open('/workspace/data/synthetic-data-1.json', 'r') as file:\n",
    "    raw_df = json.load(file)\n",
    "\n",
    "raw_df = pd.DataFrame(raw_df)\n",
    "\n",
    "input_df =\\\n",
    "    pd.concat([\n",
    "        raw_df\\\n",
    "            .rename(columns = {'clean_list': 'full_list', 'clean_category_indices': 'category_indices', 'clean_category_count': 'category_count'})\\\n",
    "            .assign(variant = 'clean')\\\n",
    "            [['sample_ix', 'variant', 'category', 'list_length', 'full_list', 'category_indices', 'category_count']],\n",
    "        raw_df\\\n",
    "            .rename(columns = {'corrupt_list': 'full_list', 'corrupt_category_indices': 'category_indices', 'corrupt_category_count': 'category_count'})\\\n",
    "            .assign(variant = 'corrupt')\\\n",
    "            [['sample_ix', 'variant', 'category', 'list_length', 'full_list', 'category_indices', 'category_count']]\n",
    "    ], ignore_index = True)\\\n",
    "    .assign(\n",
    "        category_words = lambda df: df.apply(lambda row: [row['full_list'][i] for i in row['category_indices']], axis = 1),\n",
    "    )\n",
    "\n",
    "# input_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c5024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df['sample_ix'] = input_df.groupby('variant').cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccd4c266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_ix</th>\n",
       "      <th>variant</th>\n",
       "      <th>category</th>\n",
       "      <th>list_length</th>\n",
       "      <th>full_list</th>\n",
       "      <th>category_indices</th>\n",
       "      <th>category_count</th>\n",
       "      <th>category_words</th>\n",
       "      <th>prompt</th>\n",
       "      <th>token_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>clean</td>\n",
       "      <td>colors</td>\n",
       "      <td>3</td>\n",
       "      <td>[red, banana, computer]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>1</td>\n",
       "      <td>[red]</td>\n",
       "      <td>&lt;|im_start|&gt;user\\nCount the number of words in...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>clean</td>\n",
       "      <td>vehicles</td>\n",
       "      <td>3</td>\n",
       "      <td>[car, bus, banana]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>2</td>\n",
       "      <td>[car, bus]</td>\n",
       "      <td>&lt;|im_start|&gt;user\\nCount the number of words in...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>clean</td>\n",
       "      <td>shapes</td>\n",
       "      <td>3</td>\n",
       "      <td>[square, circle, triangle]</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>3</td>\n",
       "      <td>[square, circle, triangle]</td>\n",
       "      <td>&lt;|im_start|&gt;user\\nCount the number of words in...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>clean</td>\n",
       "      <td>continents</td>\n",
       "      <td>3</td>\n",
       "      <td>[computer, asia, apple]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>1</td>\n",
       "      <td>[asia]</td>\n",
       "      <td>&lt;|im_start|&gt;user\\nCount the number of words in...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>clean</td>\n",
       "      <td>metals</td>\n",
       "      <td>3</td>\n",
       "      <td>[banana, gold, silver]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>[gold, silver]</td>\n",
       "      <td>&lt;|im_start|&gt;user\\nCount the number of words in...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7603</th>\n",
       "      <td>4982</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>games</td>\n",
       "      <td>7</td>\n",
       "      <td>[chess, rose, table, car, dog, apple, chair]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>1</td>\n",
       "      <td>[chess]</td>\n",
       "      <td>&lt;|im_start|&gt;user\\nCount the number of words in...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7604</th>\n",
       "      <td>4984</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>street_signs</td>\n",
       "      <td>7</td>\n",
       "      <td>[rose, car, yield, speed, apple, dog, pedestrian]</td>\n",
       "      <td>[2, 3, 6]</td>\n",
       "      <td>3</td>\n",
       "      <td>[yield, speed, pedestrian]</td>\n",
       "      <td>&lt;|im_start|&gt;user\\nCount the number of words in...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>4986</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>time_units</td>\n",
       "      <td>7</td>\n",
       "      <td>[apple, year, car, day, month, minute, hour]</td>\n",
       "      <td>[1, 3, 4, 5, 6]</td>\n",
       "      <td>5</td>\n",
       "      <td>[year, day, month, minute, hour]</td>\n",
       "      <td>&lt;|im_start|&gt;user\\nCount the number of words in...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>4987</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>elements</td>\n",
       "      <td>7</td>\n",
       "      <td>[hydrogen, apple, gold, silver, helium, oxygen...</td>\n",
       "      <td>[0, 2, 3, 4, 5, 6]</td>\n",
       "      <td>6</td>\n",
       "      <td>[hydrogen, gold, silver, helium, oxygen, nitro...</td>\n",
       "      <td>&lt;|im_start|&gt;user\\nCount the number of words in...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7607</th>\n",
       "      <td>4988</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>shapes</td>\n",
       "      <td>7</td>\n",
       "      <td>[apple, car, dog, rose, chair, table, house]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;|im_start|&gt;user\\nCount the number of words in...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7608 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_ix  variant      category  list_length  \\\n",
       "0             0    clean        colors            3   \n",
       "1             1    clean      vehicles            3   \n",
       "2             2    clean        shapes            3   \n",
       "3             3    clean    continents            3   \n",
       "4             4    clean        metals            3   \n",
       "...         ...      ...           ...          ...   \n",
       "7603       4982  corrupt         games            7   \n",
       "7604       4984  corrupt  street_signs            7   \n",
       "7605       4986  corrupt    time_units            7   \n",
       "7606       4987  corrupt      elements            7   \n",
       "7607       4988  corrupt        shapes            7   \n",
       "\n",
       "                                              full_list    category_indices  \\\n",
       "0                               [red, banana, computer]                 [0]   \n",
       "1                                    [car, bus, banana]              [0, 1]   \n",
       "2                            [square, circle, triangle]           [0, 1, 2]   \n",
       "3                               [computer, asia, apple]                 [1]   \n",
       "4                                [banana, gold, silver]              [1, 2]   \n",
       "...                                                 ...                 ...   \n",
       "7603       [chess, rose, table, car, dog, apple, chair]                 [0]   \n",
       "7604  [rose, car, yield, speed, apple, dog, pedestrian]           [2, 3, 6]   \n",
       "7605       [apple, year, car, day, month, minute, hour]     [1, 3, 4, 5, 6]   \n",
       "7606  [hydrogen, apple, gold, silver, helium, oxygen...  [0, 2, 3, 4, 5, 6]   \n",
       "7607       [apple, car, dog, rose, chair, table, house]                  []   \n",
       "\n",
       "      category_count                                     category_words  \\\n",
       "0                  1                                              [red]   \n",
       "1                  2                                         [car, bus]   \n",
       "2                  3                         [square, circle, triangle]   \n",
       "3                  1                                             [asia]   \n",
       "4                  2                                     [gold, silver]   \n",
       "...              ...                                                ...   \n",
       "7603               1                                            [chess]   \n",
       "7604               3                         [yield, speed, pedestrian]   \n",
       "7605               5                   [year, day, month, minute, hour]   \n",
       "7606               6  [hydrogen, gold, silver, helium, oxygen, nitro...   \n",
       "7607               0                                                 []   \n",
       "\n",
       "                                                 prompt  token_length  \n",
       "0     <|im_start|>user\\nCount the number of words in...            50  \n",
       "1     <|im_start|>user\\nCount the number of words in...            50  \n",
       "2     <|im_start|>user\\nCount the number of words in...            50  \n",
       "3     <|im_start|>user\\nCount the number of words in...            50  \n",
       "4     <|im_start|>user\\nCount the number of words in...            50  \n",
       "...                                                 ...           ...  \n",
       "7603  <|im_start|>user\\nCount the number of words in...            54  \n",
       "7604  <|im_start|>user\\nCount the number of words in...            56  \n",
       "7605  <|im_start|>user\\nCount the number of words in...            55  \n",
       "7606  <|im_start|>user\\nCount the number of words in...            54  \n",
       "7607  <|im_start|>user\\nCount the number of words in...            54  \n",
       "\n",
       "[7608 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Count the number of words in the following  list that match the given type, and put the numerical answer in parentheses.\n",
      "\n",
      "Type: colors\n",
      "List: [ red banana computer ]<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Answer: (\n"
     ]
    }
   ],
   "source": [
    "# tokenize + prep data \n",
    "# todo: move function into module\n",
    "def prep_prompt(category, choices, tokenizer):\n",
    "    choices_str = ' '.join(choices)\n",
    "    user_prompt = f\"Count the number of words in the following  list that match the given type, and put the numerical answer in parentheses.\\n\\nType: {category}\\nList: [ {choices_str} ]\"\n",
    "    \n",
    "    instruct_formatted_prompt = tokenizer.apply_chat_template(\n",
    "        [{'role': 'user', 'content': user_prompt}, {'role': 'assistant', 'content': 'Answer: ('}],\n",
    "        tokenize = False,\n",
    "        add_generation_prompt = False,\n",
    "        continue_final_message = True # Otherwise appends eos token\n",
    "    )\n",
    "\n",
    "    return instruct_formatted_prompt\n",
    "\n",
    "# Add the prompt\n",
    "input_df =\\\n",
    "    input_df\\\n",
    "    .assign(prompt = lambda df: df.apply(lambda row: prep_prompt(row['category'], row['full_list'], tokenizer), axis = 1))\n",
    "\n",
    "# Add the token lengths\n",
    "token_lengths = tokenizer(\n",
    "    input_df['prompt'].tolist(),\n",
    "    add_special_tokens = False,\n",
    "    max_length = 128,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    return_tensors = 'pt'\n",
    "    )['attention_mask'].sum(dim = 1).tolist()\n",
    "\n",
    "input_df =\\\n",
    "    input_df\\\n",
    "    .assign(token_length = token_lengths)\n",
    "\n",
    "# Filter only for sample_ix such that the two variants have the same token length, this will simplify patching\n",
    "sample_ix_with_same_length =\\\n",
    "    input_df\\\n",
    "    .groupby('sample_ix', as_index = False)\\\n",
    "    .agg(unique_lengths = ('token_length', 'nunique'))\\\n",
    "    .pipe(lambda df: df[df['unique_lengths'] == 1])\\\n",
    "    ['sample_ix']\\\n",
    "    .tolist()\n",
    "\n",
    "input_df = input_df.pipe(lambda df: df[df['sample_ix'].isin(sample_ix_with_same_length)]).reset_index(drop = True)\n",
    "\n",
    "display(input_df)\n",
    "print(input_df['prompt'].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff9c94fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(69)\n"
     ]
    }
   ],
   "source": [
    "# create dataloader\n",
    "import utils.dataset\n",
    "importlib.reload(utils.dataset)\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_prompts = tokenizer(input_df['prompt'].tolist(), add_special_tokens = False, max_length = 80, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "print(tokenized_prompts['attention_mask'].sum(dim = 1).max()) # Must be under max length to confirm nothing was truncated (since attention mask applies a 1 to indicate \"some guy was here\")\n",
    "\n",
    "# Create and chunk into lists of size 1k each - these will be the export breaks\n",
    "test_dl = DataLoader(\n",
    "    ReconstructableTextDataset(\n",
    "        input_df['prompt'].tolist(),\n",
    "        tokenizer,\n",
    "        max_length = 128,\n",
    "        sample_ix = input_df['sample_ix'].tolist(),\n",
    "        variant = input_df['variant'].tolist(),\n",
    "        full_lists = input_df['full_list'].tolist(),\n",
    "        category_indices_list = input_df['category_indices'].tolist()\n",
    "        ),\n",
    "    batch_size = 16,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ffde43",
   "metadata": {},
   "source": [
    "## Quick evals\n",
    "Identify a set of questions where both the corrupt + correct are accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eabea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 476/476 [02:44<00:00,  2.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# run fwd. pass\n",
    "output_records = []\n",
    "\n",
    "for batch_ix, batch in tqdm(enumerate(test_dl), total = len(test_dl)):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    sample_indices = batch['sample_ix']\n",
    "    variants = batch['variant']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)['logits']\n",
    "        seq_ends = attention_mask.sum(dim = 1) - 1\n",
    "\n",
    "    for i in range(len(sample_indices)):\n",
    "        last_pos = seq_ends[i].item()\n",
    "        last_logits = logits[i, last_pos, :]\n",
    "        probs = torch.softmax(last_logits, dim = -1)\n",
    "        top_token = torch.argmax(probs).item()\n",
    "        top_prob = probs[top_token].item()\n",
    "\n",
    "        output_records.append(\n",
    "            {\n",
    "                'sample_ix': int(sample_indices[i]),\n",
    "                'variant': variants[i],\n",
    "                'output_token': tokenizer.decode(top_token),\n",
    "                'output_prob': top_prob,\n",
    "            }\n",
    "        )\n",
    "\n",
    "output_df = pd.DataFrame(output_records) # todo: there's a bug w/ the data loader, causing sample_ix to repeat intra-batch..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd6d5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: fix sample index, not advised \n",
    "# input_df['sample_ix'] = input_df.groupby('variant').cumcount()\n",
    "output_df['sample_ix'] = output_df.groupby('variant').cumcount()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2a7e3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_ix</th>\n",
       "      <th>variant</th>\n",
       "      <th>category_count</th>\n",
       "      <th>category</th>\n",
       "      <th>output_token</th>\n",
       "      <th>output_prob</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>is_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>clean</td>\n",
       "      <td>1</td>\n",
       "      <td>colors</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>clean</td>\n",
       "      <td>2</td>\n",
       "      <td>vehicles</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>clean</td>\n",
       "      <td>3</td>\n",
       "      <td>shapes</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>clean</td>\n",
       "      <td>1</td>\n",
       "      <td>continents</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>clean</td>\n",
       "      <td>2</td>\n",
       "      <td>metals</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5803</th>\n",
       "      <td>3799</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>0</td>\n",
       "      <td>animals</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5804</th>\n",
       "      <td>3800</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>1</td>\n",
       "      <td>colors</td>\n",
       "      <td>2</td>\n",
       "      <td>0.816406</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5805</th>\n",
       "      <td>3801</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>2</td>\n",
       "      <td>countries</td>\n",
       "      <td>3</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5806</th>\n",
       "      <td>3802</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>0</td>\n",
       "      <td>cities</td>\n",
       "      <td>5</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5807</th>\n",
       "      <td>3803</td>\n",
       "      <td>corrupt</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "      <td>2</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5808 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_ix  variant  category_count    category output_token  \\\n",
       "0             0    clean               1      colors            1   \n",
       "1             1    clean               2    vehicles            2   \n",
       "2             2    clean               3      shapes            3   \n",
       "3             3    clean               1  continents            1   \n",
       "4             4    clean               2      metals            2   \n",
       "...         ...      ...             ...         ...          ...   \n",
       "5803       3799  corrupt               0     animals            1   \n",
       "5804       3800  corrupt               1      colors            2   \n",
       "5805       3801  corrupt               2   countries            3   \n",
       "5806       3802  corrupt               0      cities            5   \n",
       "5807       3803  corrupt               1      sports            2   \n",
       "\n",
       "      output_prob  is_correct  is_int  \n",
       "0        1.000000           1       1  \n",
       "1        1.000000           1       1  \n",
       "2        1.000000           1       1  \n",
       "3        1.000000           1       1  \n",
       "4        1.000000           1       1  \n",
       "...           ...         ...     ...  \n",
       "5803     1.000000           0       1  \n",
       "5804     0.816406           0       1  \n",
       "5805     0.996094           0       1  \n",
       "5806     0.562500           0       1  \n",
       "5807     0.996094           0       1  \n",
       "\n",
       "[5808 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy rate: 0.2185\n"
     ]
    }
   ],
   "source": [
    "# get accuracy rate for each (sample_ix, variant)\n",
    "accurate_rates =\\\n",
    "    input_df[['sample_ix', 'variant', 'category_count', 'category']]\\\n",
    "    .merge(output_df, on = ['sample_ix', 'variant'], how = 'inner')\\\n",
    "    .assign(is_correct = lambda df:  np.where(df['output_token'].str.strip() == df['category_count'].astype(str).str.strip(), 1, 0))\\\n",
    "    .assign(is_int = lambda df: np.where(df['output_token'].isin(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']), 1, 0))\n",
    "\n",
    "display(accurate_rates)\n",
    "print(f\"Accuracy rate: {accurate_rates['is_correct'].mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ba49fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517\n"
     ]
    }
   ],
   "source": [
    "# get only sample_ix's where both questions were accurate\n",
    "sample_indices_for_patching =\\\n",
    "    accurate_rates\\\n",
    "    .groupby('sample_ix', as_index = False)\\\n",
    "    .agg(n_correct = ('is_correct', 'sum'))\\\n",
    "    .pipe(lambda df: df[df['n_correct'] == 2])\\\n",
    "    ['sample_ix'].tolist()\n",
    "\n",
    "print(len(sample_indices_for_patching))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff1d13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_df = input_df.pipe(lambda df: df[df['sample_ix'].isin(sample_indices_for_patching)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3e51c",
   "metadata": {},
   "source": [
    "## Patching time\n",
    "first collect hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe5dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices_for_patching = sample_indices_for_patching[0:1000] # subset, ow will have memory problems...\n",
    "patch_df = input_df.pipe(lambda df: df[df['sample_ix'].isin(test_indices_for_patching)])\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    ReconstructableTextDataset(\n",
    "        patch_df['prompt'].tolist(),\n",
    "        tokenizer,\n",
    "        max_length = 80,\n",
    "        sample_ix = patch_df['sample_ix'].tolist(),\n",
    "        variant = patch_df['variant'].tolist(),\n",
    "        full_lists = patch_df['full_list'].tolist(),\n",
    "        category_indices_list = patch_df['category_indices'].tolist()\n",
    "        ),\n",
    "    batch_size = 16,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "978d4962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:33<00:00,  1.91it/s]\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "all_hidden_states = [] # big list of dicts; 1 entry = 1 (sample,variant)\n",
    "\n",
    "for batch_ix, batch in tqdm(enumerate(test_dl), total = len(test_dl)):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    sample_ix = batch['sample_ix'] # List\n",
    "    variant = batch['variant'] # List\n",
    "    answer_pos = batch['answer_pos'] # Tensor of length B containing the sequence positions of the answer tokens\n",
    "    list_mask = batch['list_tok_mask'].to(device) # Tensor of B x N containing a boolean mask of what tokens belong to the list\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # out includes keys 'attn_out_hidden_states', 'mlp_out_hidden_states', 'layer_out_hidden_states\n",
    "        out = run_qwen3(model, input_ids, attention_mask, return_hidden_states = True)\n",
    "    \n",
    "    out_hidden_states = torch.stack(out['layer_out_hidden_states'], dim = 0) # K x B x N x D\n",
    "    out_mlps = torch.stack(out['mlp_out_hidden_states'], dim = 0) # K x B x N x D\n",
    "    out_attns = torch.stack(out['attn_out_hidden_states'], dim = 0) # K x B x N x D\n",
    "\n",
    "    probs = torch.softmax(out['logits'], dim = -1).cpu()\n",
    "\n",
    "    for i in range(len(sample_ix)):\n",
    "        pos = answer_pos[i].item()\n",
    "        tok_id = int(torch.argmax(probs[i, pos]))\n",
    "        tok_prob = probs[i, pos, tok_id]\n",
    "        tok_str = tokenizer.decode([tok_id], skip_special_tokens = False)\n",
    "        records.append({\n",
    "            'sample_ix': int(sample_ix[i]),\n",
    "            'variant': variant[i],\n",
    "            'output_token_id': tok_id,\n",
    "            'output_token': tok_str,\n",
    "            'output_prob': tok_prob\n",
    "        })\n",
    "\n",
    "        # Now get stuff to store the hidden states plus the mapping back\n",
    "        list_mask_for_seq = list_mask[i] # N length mask\n",
    "        list_token_indices_for_seq = list_mask[i].nonzero(as_tuple = True)[0] # Only keep the indices of the list mask tensors\n",
    "        \n",
    "        layer_acts = out_hidden_states[:, i, list_token_indices_for_seq, :].detach().cpu() #  [K, T, D]\n",
    "        mlp_acts = out_mlps[:, i, list_token_indices_for_seq, :].detach().cpu() #  [K, T, D]\n",
    "        attn_acts = out_attns[:, i, list_token_indices_for_seq, :].detach().cpu() #  [K, T, D]\n",
    "\n",
    "        input_ids_seq = input_ids[i].detach().cpu()\n",
    "        attention_mask_seq = attention_mask[i].detach().cpu()\n",
    "\n",
    "        all_hidden_states.append({\n",
    "            'sample_ix': int(sample_ix[i]),\n",
    "            'variant': variant[i],\n",
    "            'token_indices': list_token_indices_for_seq,\n",
    "            'hidden_states': layer_acts,\n",
    "            'attn_states': attn_acts, \n",
    "            'mlp_states': mlp_acts,\n",
    "            \"input_ids\" : input_ids_seq,                \n",
    "            \"attention_mask\" : attention_mask_seq,\n",
    "            'is_category_tok': batch['is_category_tok'][i, list_token_indices_for_seq.cpu()],\n",
    "            'is_word_start': batch['word_start_tok_mask'][i, list_token_indices_for_seq.cpu()],\n",
    "            'running_count': batch['running_count'][i, list_token_indices_for_seq.cpu()],\n",
    "            'answer_pos': pos\n",
    "        })\n",
    "\n",
    "    # if batch_ix > 100:\n",
    "    #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63e5f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local (in case)\n",
    "records_df = pd.DataFrame(records)\n",
    "records_df[\"output_prob\"] = records_df[\"output_prob\"].apply(\n",
    "    lambda x: x.item() if isinstance(x, torch.Tensor) else x\n",
    ")\n",
    "records_df.to_parquet(\"../data/records_df.parquet\", index=False)\n",
    "torch.save(all_hidden_states, '../data/all_hiden_states.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d2318a",
   "metadata": {},
   "source": [
    "# Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b5301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline corrupt prediction: 18 (should be wrong w.r.t. true id 19 )\n"
     ]
    }
   ],
   "source": [
    "# single example \n",
    "sample_ix = records_df.sample_ix.value_counts().idxmax() \n",
    "donor = next(d for d in all_hidden_states\n",
    "                if d[\"sample_ix\"]==sample_ix and d[\"variant\"]==\"clean\")\n",
    "receiver = next(d for d in all_hidden_states\n",
    "                if d[\"sample_ix\"]==sample_ix and d[\"variant\"]==\"corrupt\")\n",
    "\n",
    "true_id = int(\n",
    "    records_df\n",
    "      .query(\"sample_ix == @sample_ix and variant == 'clean'\")\n",
    "      .output_token_id\n",
    "      .iloc[0]\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    base_logits = model(\n",
    "        receiver[\"input_ids\"].unsqueeze(0).to(device),\n",
    "        attention_mask = receiver[\"attention_mask\"].unsqueeze(0).to(device),\n",
    "        use_cache = False\n",
    "    ).logits\n",
    "baseline_pred = int(torch.argmax(base_logits[0, receiver[\"answer_pos\"]]))\n",
    "print(\"Baseline corrupt prediction:\", baseline_pred,\n",
    "      \"(should be wrong wrt true id\", true_id, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5afe3e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline corrupt prediction = 18 (true id = 19)\n",
      "layer 0: flipped = True\n",
      "layer 1: flipped = True\n",
      "layer 2: flipped = True\n",
      "layer 3: flipped = True\n",
      "layer 4: flipped = True\n",
      "layer 5: flipped = True\n",
      "layer 6: flipped = True\n",
      "layer 7: flipped = True\n",
      "layer 8: flipped = True\n",
      "layer 9: flipped = True\n",
      "layer 10: flipped = True\n",
      "layer 11: flipped = True\n",
      "layer 12: flipped = True\n",
      "layer 13: flipped = True\n",
      "layer 14: flipped = True\n",
      "layer 15: flipped = True\n",
      "layer 16: flipped = True\n",
      "layer 17: flipped = True\n",
      "layer 18: flipped = True\n",
      "layer 19: flipped = True\n",
      "layer 20: flipped = True\n",
      "layer 21: flipped = True\n",
      "layer 22: flipped = True\n",
      "layer 23: flipped = False\n",
      "layer 24: flipped = False\n",
      "layer 25: flipped = False\n",
      "layer 26: flipped = False\n",
      "layer 27: flipped = False\n",
      "layer 28: flipped = False\n",
      "layer 29: flipped = False\n",
      "layer 30: flipped = False\n",
      "layer 31: flipped = False\n",
      "layer 32: flipped = False\n",
      "layer 33: flipped = False\n",
      "layer 34: flipped = False\n",
      "layer 35: flipped = False\n",
      "layer 36: flipped = False\n",
      "layer 37: flipped = False\n",
      "layer 38: flipped = False\n",
      "layer 39: flipped = False\n"
     ]
    }
   ],
   "source": [
    "num_layers = len(model.model.layers)     \n",
    "token_idx = receiver[\"token_indices\"].to(device)   \n",
    "answer_pos = receiver[\"answer_pos\"]  \n",
    "\n",
    "def patch_layer(layer_ix: int) -> bool:\n",
    "    \"\"\"\n",
    "    True -> patching this block flips the corrupt answer to the clean one.\n",
    "    False -> answer stays wrong.\n",
    "    \"\"\"\n",
    "    donor_slice = donor[\"hidden_states\"][layer_ix].to(device)\n",
    "\n",
    "    # hook overwriting the residual stream for the list tokens\n",
    "    def hook(_, __, output):\n",
    "        if isinstance(output, tuple):       \n",
    "            hidden, *rest = output\n",
    "            hidden = hidden.clone()\n",
    "            hidden[0, token_idx] = donor_slice\n",
    "            return (hidden, *rest)\n",
    "        else:\n",
    "            hidden = output.clone() \n",
    "            hidden[0, token_idx] = donor_slice\n",
    "            return hidden\n",
    "\n",
    "    handle = model.model.layers[layer_ix].register_forward_hook(hook)\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            logits = model(\n",
    "                receiver[\"input_ids\"].unsqueeze(0).to(device),\n",
    "                attention_mask = receiver[\"attention_mask\"].unsqueeze(0).to(device),\n",
    "            ).logits\n",
    "    finally:\n",
    "        handle.remove() # it is important you remove these for mem!!!!\n",
    "        del donor_slice    \n",
    "\n",
    "    new_top = int(torch.argmax(logits[0, answer_pos]))\n",
    "    return new_top == true_id\n",
    "\n",
    "print(f\"Baseline corrupt prediction = {baseline_pred} (true id = {true_id})\")\n",
    "for L in range(num_layers):\n",
    "    flipped = patch_layer(L)\n",
    "    print(f\"layer {L}: flipped = {flipped}\")\n",
    "\n",
    "# nice, so here it looks like transplanting hidden states (block-level, token-slice patching) from clean => corrupt \"recovers\" the answer\n",
    "# (in this instance) when done to layer 0 through 22. After that point, the \"damage is done,\" so it seems. \n",
    "\n",
    "# you likely want to refine this further, identifying specific heads/neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0facd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's repeat across more samples for a minimal causal mediation analysis – we just want to initially answer \n",
    "# \"which hidden-state layer contains a representation of the running count\"\n",
    "clean_by_ix = {d[\"sample_ix\"]: d for d in all_hidden_states if d[\"variant\"]==\"clean\"}\n",
    "corrupt_by_ix = {d[\"sample_ix\"]: d for d in all_hidden_states if d[\"variant\"]==\"corrupt\"}\n",
    "\n",
    "# this contains the \"true\" token id \n",
    "true_id_by_ix = (\n",
    "    records_df\n",
    "      .query('variant == \"clean\"')\n",
    "      .set_index(\"sample_ix\")[\"output_token_id\"]\n",
    "      .astype(int)\n",
    "      .to_dict()\n",
    ")\n",
    "\n",
    "def flip_with_layer(donor, receiver, true_id, layer_ix) -> bool:\n",
    "    token_idx = receiver[\"token_indices\"].to(device)\n",
    "    ans_pos = receiver[\"answer_pos\"]\n",
    "    donor_slice = donor[\"hidden_states\"][layer_ix].to(device)\n",
    "\n",
    "    def hook(_, __, output):\n",
    "        if isinstance(output, tuple):\n",
    "            h, *rest = output\n",
    "            h = h.clone(); h[0, token_idx] = donor_slice\n",
    "            return (h, *rest)\n",
    "        else:\n",
    "            h = output.clone(); h[0, token_idx] = donor_slice\n",
    "            return h\n",
    "\n",
    "    handle = model.model.layers[layer_ix].register_forward_hook(hook)\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            logits = model(\n",
    "                receiver[\"input_ids\"].unsqueeze(0).to(device),\n",
    "                attention_mask = receiver[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "            ).logits\n",
    "    finally:\n",
    "        handle.remove()\n",
    "        del donor_slice\n",
    "\n",
    "    return int(torch.argmax(logits[0, ans_pos])) == true_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dc46dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a while since qwen14B has 40 layers that it's operating/patching across each time\n",
    "layer_hits = torch.zeros(num_layers, dtype=torch.long)  \n",
    "layer_total = torch.zeros_like(layer_hits)              \n",
    "\n",
    "for k in corrupt_by_ix:       \n",
    "    donor = clean_by_ix[k]\n",
    "    receiver = corrupt_by_ix[k]\n",
    "    true_id = true_id_by_ix[k]\n",
    "\n",
    "    # skip samples where corrupt is already correct (nothing to “flip”)\n",
    "    with torch.no_grad():\n",
    "        logits_corrupt = model(\n",
    "            receiver[\"input_ids\"].unsqueeze(0).to(device),\n",
    "            attention_mask = receiver[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "        ).logits\n",
    "    corrupt_top = int(torch.argmax(logits_corrupt[0, receiver[\"answer_pos\"]]))\n",
    "    if corrupt_top == true_id:\n",
    "        continue\n",
    "\n",
    "    for L in range(num_layers):\n",
    "        flipped = flip_with_layer(donor, receiver, true_id, L)\n",
    "        layer_total[L] += 1\n",
    "        layer_hits[L]  += int(flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "112a3df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0: flip-rate = 99.78%  (n=446)\n",
      "layer 1: flip-rate = 99.55%  (n=446)\n",
      "layer 2: flip-rate = 99.55%  (n=446)\n",
      "layer 3: flip-rate = 99.78%  (n=446)\n",
      "layer 4: flip-rate = 99.33%  (n=446)\n",
      "layer 5: flip-rate = 99.78%  (n=446)\n",
      "layer 6: flip-rate = 99.33%  (n=446)\n",
      "layer 7: flip-rate = 99.78%  (n=446)\n",
      "layer 8: flip-rate = 99.78%  (n=446)\n",
      "layer 9: flip-rate = 99.55%  (n=446)\n",
      "layer 10: flip-rate = 99.55%  (n=446)\n",
      "layer 11: flip-rate = 99.33%  (n=446)\n",
      "layer 12: flip-rate = 99.33%  (n=446)\n",
      "layer 13: flip-rate = 99.10%  (n=446)\n",
      "layer 14: flip-rate = 99.10%  (n=446)\n",
      "layer 15: flip-rate = 98.65%  (n=446)\n",
      "layer 16: flip-rate = 98.43%  (n=446)\n",
      "layer 17: flip-rate = 97.76%  (n=446)\n",
      "layer 18: flip-rate = 96.19%  (n=446)\n",
      "layer 19: flip-rate = 93.50%  (n=446)\n",
      "layer 20: flip-rate = 84.08%  (n=446)\n",
      "layer 21: flip-rate = 75.34%  (n=446)\n",
      "layer 22: flip-rate = 69.51%  (n=446)\n",
      "layer 23: flip-rate = 53.14%  (n=446)\n",
      "layer 24: flip-rate = 16.82%  (n=446)\n",
      "layer 25: flip-rate = 6.05%  (n=446)\n",
      "layer 26: flip-rate = 0.90%  (n=446)\n",
      "layer 27: flip-rate = 0.22%  (n=446)\n",
      "layer 28: flip-rate = 0.22%  (n=446)\n",
      "layer 29: flip-rate = 0.00%  (n=446)\n",
      "layer 30: flip-rate = 0.00%  (n=446)\n",
      "layer 31: flip-rate = 0.00%  (n=446)\n",
      "layer 32: flip-rate = 0.00%  (n=446)\n",
      "layer 33: flip-rate = 0.00%  (n=446)\n",
      "layer 34: flip-rate = 0.00%  (n=446)\n",
      "layer 35: flip-rate = 0.00%  (n=446)\n",
      "layer 36: flip-rate = 0.00%  (n=446)\n",
      "layer 37: flip-rate = 0.00%  (n=446)\n",
      "layer 38: flip-rate = 0.00%  (n=446)\n",
      "layer 39: flip-rate = 0.00%  (n=446)\n"
     ]
    }
   ],
   "source": [
    "flip_rate = (layer_hits / layer_total.float()).tolist()\n",
    "for L, rate in enumerate(flip_rate):\n",
    "    print(f\"layer {L}: flip-rate = {rate:5.2%}  (n={layer_total[L]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "895cff7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Layer %{x}<br>Flip-rate %{y:.1%}<extra></extra>",
         "mode": "lines+markers",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39
         ],
         "y": [
          0.9977578520774841,
          0.9955157041549683,
          0.9955157041549683,
          0.9977578520774841,
          0.9932735562324524,
          0.9977578520774841,
          0.9932735562324524,
          0.9977578520774841,
          0.9977578520774841,
          0.9955157041549683,
          0.9955157041549683,
          0.9932735562324524,
          0.9932735562324524,
          0.9910314083099365,
          0.9910314083099365,
          0.9865471124649048,
          0.9843049049377441,
          0.9775784611701965,
          0.9618834257125854,
          0.9349775910377502,
          0.8408071994781494,
          0.7533632516860962,
          0.695067286491394,
          0.5313901305198669,
          0.16816143691539764,
          0.060538116842508316,
          0.008968610316514969,
          0.0022421525791287422,
          0.0022421525791287422,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ]
        }
       ],
       "layout": {
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Running-count mediation (full sweep)"
        },
        "xaxis": {
         "title": {
          "text": "Layer"
         }
        },
        "yaxis": {
         "range": [
          0,
          1
         ],
         "title": {
          "text": "Flip-rate (prop. rescued :))"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "layers = list(range(len(flip_rate)))\n",
    "\n",
    "x  = [l for l, r in zip(layers, flip_rate)]\n",
    "y  = [r for r in flip_rate]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = x,\n",
    "    y = y,\n",
    "    mode = \"lines+markers\",\n",
    "    hovertemplate = \"Layer %{x}<br>Flip-rate %{y:.1%}<extra></extra>\"\n",
    "))\n",
    "fig.update_layout(\n",
    "    title = \"Running-count mediation (full sweep)\",\n",
    "    xaxis_title = \"Layer\",\n",
    "    yaxis_title = \"Flip-rate (prop. rescued :))\",\n",
    "    yaxis_range = [0, 1],\n",
    "    paper_bgcolor=\"white\",   \n",
    "    plot_bgcolor=\"white\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d99659a",
   "metadata": {},
   "source": [
    "## Probe sweep - pick the mediator layer $l^*$\n",
    "won't do (right now), no time (right now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683fd5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.metrics import r2_score\n",
    "# import random \n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 0 — collect clean-only records into per-layer bags\n",
    "# # ------------------------------------------------------------\n",
    "# K = output_records[0][\"hidden_states\"].shape[0]   # # layers\n",
    "# rng = random.Random(42)\n",
    "# # MAX_ROWS = 10_000 # Per split, per\n",
    "# alpha = 10 # Choose such that tr/test loss is similar\n",
    "\n",
    "# # Split by sample_ix so paired clean/corrupt never leak\n",
    "# clean_records = [r for r in output_records if r[\"variant\"] == \"clean\"]\n",
    "# sample_ids = sorted({r[\"sample_ix\"] for r in clean_records})\n",
    "# rng.shuffle(sample_ids)\n",
    "# split = int(0.8 * len(sample_ids))\n",
    "# train_ids, test_ids = set(sample_ids[:split]), set(sample_ids[split:])\n",
    "\n",
    "# K = clean_records[0][\"hidden_states\"].shape[0]\n",
    "# bags = {l: {\"train_X\": [], \"train_y\": [], \"test_X\": [], \"test_y\": []} for l in range(K)}\n",
    "\n",
    "# for rec in clean_records:\n",
    "#     bag = (\n",
    "#         \"train_\" if rec[\"sample_ix\"] in train_ids else \"test_\"\n",
    "#     )\n",
    "#     hs = rec[\"hidden_states\"].numpy()        # K × Nₗ × D  (already CPU)\n",
    "#     y  = rec[\"cum_count\"].numpy()            # Nₗ\n",
    "#     for l in range(K):\n",
    "#         bags[l][bag + \"X\"].append(hs[l])     # each is (Nₗ, D)\n",
    "#         bags[l][bag + \"y\"].append(y)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 1 — stack into 2-D matrices per layer\n",
    "# # ------------------------------------------------------------\n",
    "# Xs_tr, ys_tr, Xs_te, ys_te = [], [], [], []\n",
    "# for l in range(K):\n",
    "#     X_tr = np.concatenate(bags[l][\"train_X\"])   # (∑Nₗ, D)\n",
    "#     y_tr = np.concatenate(bags[l][\"train_y\"])\n",
    "#     X_te = np.concatenate(bags[l][\"test_X\"])\n",
    "#     y_te = np.concatenate(bags[l][\"test_y\"])\n",
    "\n",
    "#     Xs_tr.append(X_tr)\n",
    "#     ys_tr.append(y_tr)\n",
    "#     Xs_te.append(X_te)\n",
    "#     ys_te.append(y_te)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 2 — train one Ridge probe per layer, evaluate on held-out set\n",
    "# # ------------------------------------------------------------\n",
    "# r2_test, probes = [], []\n",
    "# for l in range(K):\n",
    "#     reg = Ridge(alpha = alpha, solver = 'auto', fit_intercept=True)\n",
    "#     reg.fit(Xs_tr[l], ys_tr[l])\n",
    "\n",
    "#     y_hat_tr = reg.predict(Xs_tr[l])\n",
    "#     y_hat_te = reg.predict(Xs_te[l])\n",
    "#     R2_train = r2_score(reg.predict(Xs_tr[l]),  ys_tr[l])\n",
    "\n",
    "#     r2_tr = r2_score(ys_tr[l], y_hat_tr)\n",
    "#     r2_te = r2_score(ys_te[l], y_hat_te)\n",
    "\n",
    "#     r2_test.append(r2_te)\n",
    "#     probes.append(reg)\n",
    "\n",
    "#     print(f\"Layer {l:2d} | train R² {r2_tr:6.3f} | test R² {r2_te:6.3f}\")\n",
    "\n",
    "# print(\"\\nLayer  |  Test R²\")\n",
    "# for l, r2 in enumerate(r2_test):\n",
    "#     print(f\"{l:2d}     {r2:6.3f}\")\n",
    "\n",
    "# layer_star = int(np.argmax(r2_test))\n",
    "# print(f\"\\nMediator candidate layer = {layer_star}  (R² = {r2_test[layer_star]:.3f})\")\n",
    "\n",
    "# w_star = torch.tensor(probes[layer_star].coef_, dtype=torch.float32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
