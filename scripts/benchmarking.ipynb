{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e1d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing to see if models are capable of counting task \n",
    "# w/ performance > random chance\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import importlib\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.evals import load_model, create_counting_prompt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d7b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSISTENT_MODEL_DIR = \"/workspace/models\"\n",
    "DATA_DIR = \"/workspace/data/synthetic-data.json\"\n",
    "\n",
    "model_dirs = [\n",
    "    PERSISTENT_MODEL_DIR + '/Qwen3-1.7B',\n",
    "    PERSISTENT_MODEL_DIR + '/Qwen3-4B',\n",
    "    PERSISTENT_MODEL_DIR + '/Qwen3-8B',\n",
    "    PERSISTENT_MODEL_DIR + '/Qwen3-14B',\n",
    "    PERSISTENT_MODEL_DIR + '/Phi-3-mini-4k-instruct' # ideally, we'll later use this to check for generality\n",
    "]\n",
    "\n",
    "# import benchmarking dataset \n",
    "count_df = pd.read_json(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582d449",
   "metadata": {},
   "source": [
    "# Formatting \n",
    "Get test data into appropriate form, including instruct formatting. \n",
    "\n",
    "Create a torch dataset to hold all test examples + use dataloader or efficient batched inference during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dde26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(utils.evals)\n",
    "from utils.evals import load_model, create_counting_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f3942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model \n",
    "tok, model = load_model(dir = model_dirs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110fcfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_counting_prompt('fruit', ['banana', 'dog', 'animal', 'apple', 'guava'], tok, device)\n",
    "pprint(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tok(test, return_tensors = 'pt').to(device)\n",
    "ids\n",
    "out = model(input_ids = ids['input_ids'], attention_mask = ids['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fe8e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out['logits'].shape # 1 x 64 \n",
    "\n",
    "predicted_token_ids = torch.argmax(out['logits'][0], dim = -1)\n",
    "predicted_text = tok.decode(predicted_token_ids, skip_special_tokens=False)\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd4316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset into a dataloader â€“ this will help handle batching\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, q_indices, questions, choices, subjects, answer_chars, tokenized_prompts):\n",
    "        self.q_indices = q_indices\n",
    "        self.questions = questions\n",
    "        self.choices = choices\n",
    "        self.subjects = subjects\n",
    "        self.answer_chars = answer_chars\n",
    "        self.input_ids = tokenized_prompts['input_ids']\n",
    "        self.attention_mask = tokenized_prompts['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'q_indices': self.q_indices[idx],\n",
    "            'questions': self.questions[idx],\n",
    "            'choices': self.choices[idx],\n",
    "            'subjects': self.subjects[idx],\n",
    "            'answer_chars': self.answer_chars[idx],\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx]\n",
    "        }\n",
    "\n",
    "# we're tokenizing everything here, but the dataloader will handle batching later :) \"tokenizing is very cheap\"\n",
    "tokenized_prompts = tokenizer(q_df['input_prompt'].tolist(), add_special_tokens = False, max_length = 400, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "print(tokenized_prompts['attention_mask'].sum(dim = 1).max()) # Must be under max length to confirm nothing was truncated (since attention mask applies a 1 to indicate \"some guy was here\")\n",
    "\n",
    "q_dl = DataLoader(TextDataset(\n",
    "    q_df['q_ix'].tolist(),\n",
    "    q_df['question'].tolist(),\n",
    "    q_df['choices'].tolist(),\n",
    "    q_df['subject'].tolist(),\n",
    "    q_df['answer_char'].tolist(),\n",
    "    tokenized_prompts # don't move to gpu yet (or will have big mem problems)\n",
    "), batch_size = 4, shuffle = False)\n",
    "\n",
    "# pprint.pp(next(iter(mmlu_dl)), width = 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909a5add",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7dc7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chance accuracy is expected prob. of a uniform random guess \n",
    "# being correct, averaged over dataset (since lists are of varying lengths)\n",
    "\n",
    "\n",
    "# want to check accuracy for count + instruction following (does it output what we expect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee087ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting: performance across model family"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
