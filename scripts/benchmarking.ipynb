{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "65e1d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing to see if models are capable of counting task \n",
    "# w/ performance > random chance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import importlib\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.evals import load_model, create_counting_prompt\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d2d7b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSISTENT_MODEL_DIR = \"/workspace/models/\"\n",
    "DATA_DIR = \"/workspace/data/synthetic-data.json\"\n",
    "MODEL_PREFIX = [\n",
    "    'Qwen3-1.7B',\n",
    "    'Qwen3-4B',\n",
    "    'Qwen3-8B',\n",
    "    'Qwen3-14B',\n",
    "    'Phi-3-mini-4k-instruct' # ideally, we'll later use this to check for generality\n",
    "][0]\n",
    "\n",
    "# import benchmarking dataset \n",
    "count_df = pd.read_json(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582d449",
   "metadata": {},
   "source": [
    "# Formatting \n",
    "Get test data into appropriate form, including instruct formatting. \n",
    "\n",
    "Create a torch dataset to hold all test examples + use dataloader or efficient batched inference during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b1a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564294f375e14c6ca11b2112945cdca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer, model = load_model(PERSISTENT_MODEL_DIR + MODEL_PREFIX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5133a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df['full_list_string'] = [' '.join(ele) for ele in count_df['full_list']]\n",
    "\n",
    "count_df['prompt'] = count_df.apply(\n",
    "    lambda row: create_counting_prompt(\n",
    "        entity_type = row['category'],\n",
    "        word_list = row['full_list_string'],\n",
    "        tokenizer = tokenizer\n",
    "    ), axis = 1\n",
    ")\n",
    "\n",
    "# pprint(count_df['prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd4316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(69)\n"
     ]
    }
   ],
   "source": [
    "# load dataset into a dataloader – this will help handle batching\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sample_index, tokenized_prompts):\n",
    "        self.sample_index = sample_index\n",
    "        self.input_ids = tokenized_prompts['input_ids']\n",
    "        self.attention_mask = tokenized_prompts['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sample_index': self.sample_index[idx],\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx]\n",
    "        }\n",
    "\n",
    "# we're tokenizing everything here, but the dataloader will handle batching later :) \"tokenizing is very cheap\"\n",
    "tokenized_prompts = tokenizer(count_df['prompt'].tolist(), add_special_tokens = False, max_length = 100, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "print(tokenized_prompts['attention_mask'].sum(dim = 1).max()) # Must be under max length to confirm nothing was truncated (since attention mask applies a 1 to indicate \"some guy was here\")\n",
    "\n",
    "count_dl = DataLoader(TextDataset(\n",
    "    count_df['sample_ix'].tolist(),\n",
    "    tokenized_prompts # don't move to gpu yet (or will have big mem problems)\n",
    "), batch_size = 4, shuffle = False)\n",
    "\n",
    "# pprint(next(iter(count_dl)), width = 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909a5add",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b646c430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:11<00:00, 10.70it/s]\n"
     ]
    }
   ],
   "source": [
    "k = 1 \n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(count_dl, total = len(count_dl)):\n",
    "        sample_ix = batch[\"sample_index\"]\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attn_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        probs = model(input_ids, attention_mask=attn_mask).logits.softmax(-1)\n",
    "\n",
    "        last_pos = attn_mask.sum(1) - 1\n",
    "        b_idx = torch.arange(input_ids.size(0), device = device)\n",
    "\n",
    "        tok_ids = input_ids[b_idx, last_pos]\n",
    "        tok_probs = probs[b_idx, last_pos, :]\n",
    "\n",
    "        \n",
    "        topk_probs, topk_idx = torch.topk(tok_probs, k = k, dim = 1)\n",
    "        tokens_flat = tokenizer.convert_ids_to_tokens(topk_idx.cpu().flatten().tolist())\n",
    "        topk_tokens = [tokens_flat[i * k:(i + 1) * k] for i in range(len(tokens_flat) // k)]\n",
    "\n",
    "        for s_ix, toks, ps in zip(sample_ix.tolist(), topk_tokens, topk_probs.cpu()):\n",
    "            results.append(\n",
    "                {\n",
    "                    \"sample_ix\": s_ix,\n",
    "                    \"output_token\": toks[0], # this code only supports k = 1 for now            \n",
    "                    \"output_prob\": ps.tolist()[0]\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)\n",
    "\n",
    "# join with the original dataframe to get accuracy \n",
    "combined = pd.merge(results, count_df, on = 'sample_ix', how = 'inner')\n",
    "\n",
    "# identify the # correct \n",
    "combined['is_correct'] = np.where(combined['output_token'] == combined['category_length'], 1, 0)\n",
    "\n",
    "# # filter for rows where is_correct = 1 – oh boy it never gets this right for 1.7\n",
    "# combined.query(\"is_correct == 1\")\n",
    "# combined['is_correct'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7dc7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17511708028636683\n"
     ]
    }
   ],
   "source": [
    "##  accuracy metrics \n",
    "# todo: want to check accuracy for count + instruction following (does it output what we expect) – generally answer format following seems to be pretty good (even w/ 1.7B)\n",
    "# todo: also want to check answer plausibility (basically, does the model say \"4\" when the full list is only 3)\n",
    "# chance accuracy is expected prob. of a uniform random guess \n",
    "# being correct, averaged over dataset (since lists are of varying lengths)\n",
    "\n",
    "# overall accuracy \n",
    "acc = combined['is_correct'].mean()\n",
    "\n",
    "# check what % are returning integers \n",
    "\n",
    "# expected accuracy under a uniform random guess\n",
    "chance = (1/combined['full_list_length']).mean()\n",
    "\n",
    "print(f\"Accuracy : {acc:.3%}  ({df['is_correct'].sum()}/{len(df)})\")\n",
    "print(f\"Chance accuracy: {chance:.3%}\")\n",
    "print(f\"Accuracy lift  : {acc - chance:+.3%}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f489906f",
   "metadata": {},
   "source": [
    "## Plot \n",
    "1. Model size (params) v. accuracy \n",
    "2. Total length, accuracy (check to see how accuracy changes as list length changes)\n",
    "3. Correct length (x), total length (y) – a heatmap\n",
    "\n",
    "correct length, total length, accuracy "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
